{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870de644-fad7-44de-be33-41a05208b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature columns\n",
    "X = baseline_pm_scaled.drop('PLUS_MINUS', axis=1)\n",
    "\n",
    "# target column\n",
    "y = baseline_pm_scaled['PLUS_MINUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f24f9-ca2a-4f6a-9720-4f7d3410e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LOO expanding window\n",
    "n_splits = len(X) - 1 \n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# initialize model\n",
    "glm = LinearRegression()\n",
    "\n",
    "# storage for predictions and RMSE\n",
    "predictions = []\n",
    "rmse_scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# expanding window\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model on training data and predict on test data\n",
    "    glm.fit(X_train, y_train)\n",
    "    prediction = glm.predict(X_test)\n",
    "    predictions.append(prediction[0])\n",
    "    \n",
    "    # evaluate model\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4ab9c-6af3-435c-9016-cb9d62676629",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for expanding window\n",
    "initial_train_size = 100  # starting size of the training set\n",
    "test_size = 1             # leave-one-out (LOO) cross-validation\n",
    "target_col = 'TOTAL_PTS'  # target column name\n",
    "df = baseline_pts_scaled  # data set to use\n",
    "\n",
    "# storage for predictions and RMSE\n",
    "predictions = []\n",
    "rmse_scores = []\n",
    "\n",
    "for train_indices, test_indices in utl.expanding_window_ts_split(\n",
    "    df, initial_train_size, test_size=test_size):\n",
    "\n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_indices].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_indices][target_col]\n",
    "    X_test = df.iloc[test_indices].drop(columns=target_col)\n",
    "    y_test = df.iloc[test_indices][target_col]\n",
    "    \n",
    "    # fit model on training data and predict on test data\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    prediction = lin_reg.predict(X_test)\n",
    "    predictions.extend(prediction)\n",
    "    \n",
    "    # get RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37b933-392c-48e6-a194-1dea2fa7dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average RMSE over all splits\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(f\"Average RMSE: {average_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e8988-1774-468c-8204-d2f94601f036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63962aa9-ff16-467e-8160-abf8bdfa36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for expanding window\n",
    "initial_train_size = 100  # starting size of the training set\n",
    "test_size = 1             # leave-one-out (LOO) cross-validation\n",
    "target_col = 'PLUS_MINUS' # target column name\n",
    "df = baseline_pm_scaled   # data set to use\n",
    "\n",
    "# storage for predictions and RMSE\n",
    "predictions = []\n",
    "rmse_scores = []\n",
    "\n",
    "for train_indices, test_indices in utl.expanding_window_ts_split(\n",
    "    df, initial_train_size, test_size=test_size):\n",
    "\n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_indices].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_indices][target_col]\n",
    "    X_test = df.iloc[test_indices].drop(columns=target_col)\n",
    "    y_test = df.iloc[test_indices][target_col]\n",
    "\n",
    "    # fit model on training data and predict on test data\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    prediction = lin_reg.predict(X_test)\n",
    "    predictions.extend(prediction)\n",
    "    \n",
    "    # get RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2458df7-4d09-472d-8102-f1885bd6674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average RMSE over all splits\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(f\"Average RMSE: {average_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94434327-e732-45d8-8d86-2400fd6dc93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe8c90-7f41-45bf-995c-303f6ee9d93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276d41d-15b4-4342-a044-bd4414744e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for expanding window\n",
    "initial_train_size = 30  # starting size of the training set (to ensure some diversity of class labels)\n",
    "test_size = 1  # leave-one-out (LOO) cross-validation\n",
    "target_col = 'GAME_RESULT'  # target column name\n",
    "df = baseline_res_scaled # data set to use\n",
    "\n",
    "# storage for predictions and true labels\n",
    "prob_predictions = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over expanding window splits\n",
    "for train_indices, test_indices in utl.expanding_window_ts_split(\n",
    "    baseline_res_scaled, initial_train_size, test_size, ensure_diversity=True, target_col=target_col):\n",
    "\n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_indices].drop(columns=target_col)\n",
    "    X_test = df.iloc[test_indices].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_indices][target_col]\n",
    "    y_test = df.iloc[test_indices][target_col]\n",
    "\n",
    "    # train the logistic regression model\n",
    "    log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # predict the probability for the class of interest (assuming class 1 is the positive class)\n",
    "    proba = log_reg.predict_proba(X_test)[:, 1] # probability of class 1\n",
    "    prob_predictions.extend(proba)\n",
    "\n",
    "    # collect true labels for metrics calculations\n",
    "    y_true.extend(y_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739120c-f946-452f-aa21-d33852da07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall metrics\n",
    "\n",
    "threshold = 0.5  # threshold for classifying probabilities into binary predictions\n",
    "pred_labels = [1 if p > threshold else 0 for p in prob_predictions]\n",
    "average_accuracy = accuracy_score(y_true, pred_labels)\n",
    "overall_auc = roc_auc_score(y_true, prob_predictions)\n",
    "average_f1_score = f1_score(y_true, pred_labels)\n",
    "\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "print(f\"Overall AUC: {overall_auc:.2f}\")\n",
    "print(f\"Average F1 Score: {average_f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc198d0f-aaa6-4804-a5d7-4f869ea8edf7",
   "metadata": {},
   "source": [
    "# OLD ROllING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c328923-ec4b-45d5-a8cc-cfa1c34b79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for rolling window\n",
    "train_size = 500  # training window size\n",
    "test_size = 1     # leave-one-out (LOO) cross-validation\n",
    "target_col = 'TOTAL_PTS'  # target column name\n",
    "df = baseline_pts_scaled # data set to use\n",
    "\n",
    "# storage for predictions and RMSE scores\n",
    "predictions = []\n",
    "rmse_scores = []\n",
    "\n",
    "# rolling window\n",
    "for train_index, test_index in utl.rolling_window_ts_split(df, train_size, test_size):\n",
    "    \n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_index].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_index][target_col]\n",
    "    X_test = df.iloc[test_index].drop(columns=target_col)\n",
    "    y_test = df.iloc[test_index][target_col]\n",
    "\n",
    "    # train the linear regression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # predict the next value and store the prediction\n",
    "    prediction = lin_reg.predict(X_test)\n",
    "    predictions.extend(prediction)\n",
    "\n",
    "    # evaluate model for this prediction\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed7227-7852-4f38-98da-6ab6d7e73715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average RMSE over all predictions\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(f\"Average RMSE: {average_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c28611-2e9f-491a-b531-edeb8f48f343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2c92f-fd6a-410a-82c6-daf6a71cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for rolling window\n",
    "train_size = 500  # training window size\n",
    "test_size = 1     # leave-one-out (LOO) cross-validation\n",
    "target_col = 'PLUS_MINUS'  # target column name\n",
    "df = baseline_pm_scaled # data set to use\n",
    "\n",
    "# storage for predictions and RMSE scores\n",
    "predictions = []\n",
    "rmse_scores = []\n",
    "\n",
    "# rolling window\n",
    "for train_index, test_index in utl.rolling_window_ts_split(df, train_size, test_size):\n",
    "    \n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_index].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_index][target_col]\n",
    "    X_test = df.iloc[test_index].drop(columns=target_col)\n",
    "    y_test = df.iloc[test_index][target_col]\n",
    "\n",
    "    # train the linear regression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # predict the next value and store the prediction\n",
    "    prediction = lin_reg.predict(X_test)\n",
    "    predictions.extend(prediction)\n",
    "\n",
    "    # evaluate model for this prediction\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c726f-8740-40f8-9a16-f8ea3c7d013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average RMSE over all predictions\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(f\"Average RMSE: {average_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a90e4-7dbc-4a2b-90db-f798e1010318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f79bb4-c4ec-4fcd-a57d-7b93ab85e098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09624624-4e09-4cd7-b33b-4462d801d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# configuration for rolling window\n",
    "train_size = 500  # training window size\n",
    "test_size = 1     # leave-one-out (LOO) cross-validation\n",
    "target_col = 'GAME_RESULT'  # target column name\n",
    "df = baseline_res_scaled # data set to use\n",
    "\n",
    "# storage for predictions and true labels\n",
    "prob_predictions = []\n",
    "y_true = []\n",
    "\n",
    "# modify the call to rolling_window_ts_split to ensure class diversity for logistic regression\n",
    "for train_index, test_index in utl.rolling_window_ts_split(\n",
    "    df, train_size, test_size, ensure_diversity=True, target_col=target_col):\n",
    "\n",
    "    # get training and testing data for this window\n",
    "    X_train = df.iloc[train_index].drop(columns=target_col)\n",
    "    y_train = df.iloc[train_index][target_col]\n",
    "    X_test = df.iloc[test_index].drop(columns=target_col)\n",
    "    y_test = df.iloc[test_index][target_col]\n",
    "\n",
    "    # train the logistic regression model\n",
    "    log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # predict the probability for the class of interest (assuming class 1 is the positive class)\n",
    "    proba = log_reg.predict_proba(X_test)[:, 1]  # probability of class 1\n",
    "    prob_predictions.extend(proba)\n",
    "\n",
    "    # collect true labels for metrics calculations\n",
    "    y_true.extend(y_test)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0acc8-2e1d-41f1-8171-27e7a89fa52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall metrics\n",
    "\n",
    "threshold = 0.5  # threshold for classifying probabilities into binary predictions\n",
    "pred_labels = [1 if p > threshold else 0 for p in prob_predictions]\n",
    "average_accuracy = accuracy_score(y_true, pred_labels)\n",
    "overall_auc = roc_auc_score(y_true, prob_predictions)\n",
    "average_f1_score = f1_score(y_true, pred_labels)\n",
    "\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "print(f\"Overall AUC: {overall_auc:.2f}\")\n",
    "print(f\"Average F1 Score: {average_f1_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
